{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loading data for reviews and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the reviews dataset for Gift Cards\n",
    "gift_cards_reviews = load_dataset(\n",
    "    \"McAuley-Lab/Amazon-Reviews-2023\",\n",
    "    name=\"raw_review_Gift_Cards\",\n",
    "    split=\"full\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load the metadata dataset for Gift Cards\n",
    "gift_cards_metadata = load_dataset(\n",
    "    \"McAuley-Lab/Amazon-Reviews-2023\",\n",
    "    name=\"raw_meta_Gift_Cards\",\n",
    "    split=\"full\",\n",
    "    trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Processing data(use parent_asin to link reviews and metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Amazon Reload', 'text': 'Having Amazon money is always good.'}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Step 1: Create a dictionary to map parent_asin to titles\n",
    "metadata_by_asin = {meta[\"parent_asin\"]: meta[\"title\"] for meta in gift_cards_metadata}\n",
    "\n",
    "# Step 2: Combine reviews with metadata\n",
    "combined_data = {\n",
    "    \"title\": [],\n",
    "    \"text\": [],\n",
    "}\n",
    "\n",
    "for review in gift_cards_reviews:\n",
    "    parent_asin = review.get(\"parent_asin\")  # Get parent_asin from review\n",
    "    if parent_asin in metadata_by_asin:\n",
    "        combined_data[\"title\"].append(metadata_by_asin[parent_asin])  # Match title using parent_asin\n",
    "        combined_data[\"text\"].append(review[\"text\"])  # Add review text\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convert combined data to a Hugging Face Dataset\n",
    "processed_dataset = Dataset.from_dict(combined_data)\n",
    "\n",
    "# Preview the processed dataset\n",
    "print(processed_dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Testing for data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Amazon Reload', 'text': 'Having Amazon money is always good.'}\n",
      "{'title': 'Amazon.com Gift Card in a Black Gift Box (Thank You Card Design)', 'text': \"Always the perfect gift.  I have never given one and had someone seem or act disappointed.  Just the opposite.  They are thrilled and excited to have a bit of a spree.  Always the perfect size and color!  Arrives in 1 day in most cases.  So it's never too late!  Lots of cards to chose from... thank you... birthday... wedding..baby..  and many that work for many occasions...\"}\n",
      "{'title': 'Amazon.com Gift Card in a Gift Box (Various Thank You Designs)', 'text': \"When you have a person who is hard to shop for.. an amazon gift card is P E R F E C T.  Man or woman...  No matter what their hobby... lifestyle.. or age.  All you have to do is pick the $.  Don't forget to mention that it is a GIFT when you check out - you will have some gift card options.  I've ordered many of these over years.  They are always received with glee.  Woo hoo!  If you're looking for a great fit for me - this is just my size!  :)  Best to all!\"}\n"
     ]
    }
   ],
   "source": [
    "# Test few rows to make sure data is processed correctly\n",
    "\n",
    "for i in range(3):\n",
    "    print(processed_dataset[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Import language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: EleutherAI/gpt-neo-125m\n",
      "Model parameters: 125198592\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Define arguments for loading the model\n",
    "class args:\n",
    "    model_name_or_path = \"EleutherAI/gpt-neo-125m\"\n",
    "    cache_dir = \"./cache/\"\n",
    "    model_revision = \"main\"\n",
    "    use_fast_tokenizer = True\n",
    "\n",
    "# Load model configuration\n",
    "config = AutoConfig.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    cache_dir=args.cache_dir,\n",
    "    revision=args.model_revision,\n",
    "    use_auth_token=None,  # No authentication token required for this public model\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    cache_dir=args.cache_dir,\n",
    "    use_fast=args.use_fast_tokenizer,\n",
    "    revision=args.model_revision,\n",
    "    use_auth_token=None,\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in args.model_name_or_path),  # Handle TensorFlow checkpoints if applicable\n",
    "    config=config,\n",
    "    cache_dir=args.cache_dir,\n",
    "    revision=args.model_revision,\n",
    "    use_auth_token=None,\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "print(f\"Model loaded: {args.model_name_or_path}\")\n",
    "print(f\"Model parameters: {model.num_parameters()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Prepare and check final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f0b4d2d31544fe8dcf20278d562b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/152410 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample entry from train_dataset:\n",
      "{'title': 'Amazon Reload', 'text': '<Product Title>: Amazon Reload <Review>: Having Amazon money is always good.'}\n",
      "Number of entries in train_dataset: 152410\n"
     ]
    }
   ],
   "source": [
    "# Assign padding token if not already set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Combine title and text if the dataset isn't preprocessed\n",
    "def combine_columns(example):\n",
    "    return {\"text\": f\"<Product Title>: {example['title']} <Review>: {example['text']}\"}\n",
    "\n",
    "# Preprocess the dataset\n",
    "train_dataset = processed_dataset.map(combine_columns)\n",
    "\n",
    "# Check the train dataset\n",
    "print(\"Sample entry from train_dataset:\")\n",
    "print(train_dataset[0])\n",
    "\n",
    "# Verify the number of entries\n",
    "print(f\"Number of entries in train_dataset: {len(train_dataset)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Customize training arguments\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"adamw_torch\",\n",
    "    save_steps=10,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=50,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
